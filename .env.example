# Example configuration file for the microservices demo
# Copy this file and modify values as needed

# RabbitMQ Configuration
RABBITMQ_URL=amqp://guest:guest@localhost:5672/
RABBITMQ_HOST=localhost
RABBITMQ_PORT=5672
RABBITMQ_USERNAME=guest
RABBITMQ_PASSWORD=guest

# API Server Configuration
API_SERVER_PORT=8081
API_SERVER_HOST=localhost

# Frontend Configuration
FRONTEND_PORT=8080
FRONTEND_HOST=localhost

# Job Runner Configuration (Legacy - now AI Research Agent)
# JOB_PROCESSING_MIN_DURATION=5
# JOB_PROCESSING_MAX_DURATION=60
# JOB_TIMEOUT_SECONDS=60
# JOB_SUCCESS_RATE=0.9

# AI Research Agent Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
DAPR_HTTP_ENDPOINT=http://localhost:3500

# AI Model Configuration (Local Ollama)
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_ORIGINS=*

# External Ollama Configuration (uncomment if using external server)
# OLLAMA_URL=http://your-ollama-server:11434
# OLLAMA_HOST=your-ollama-server
# OLLAMA_PORT=11434

# Development Configuration
GIN_MODE=debug
LOG_LEVEL=info

# Production Configuration (uncomment for production)
# GIN_MODE=release
# LOG_LEVEL=warn
